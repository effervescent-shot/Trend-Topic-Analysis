{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "import gensim\n",
    "\n",
    "\n",
    "import preprocessor as p\n",
    "from preprocessor.api import clean\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: before saving the script try cleaning with RT @... and other punctiation stuff \n",
    "# may be # can stay it's a bit problematic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-07-26_trends.csv',\n",
       " '2019-07-18_trends.csv',\n",
       " '2019-07-20_trends.csv',\n",
       " '2019-07-07_trends.csv',\n",
       " '2019-07-12_trends.csv']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../Data\"\n",
    "TWEETS_PATH = os.path.join(DATA_DIR, 'tweets')\n",
    "TREND_PATH = os.path.join(DATA_DIR, 'trends')\n",
    "SAVE_PATH = os.path.join(DATA_DIR, 'save')\n",
    "os.listdir(SAVE_PATH)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( os.path.join(SAVE_PATH,\"2019-07-26_trends.csv\"),\n",
    "                header=0, usecols=[4,5,6,7], parse_dates=['trend_date'])\n",
    "#Take only english ones\n",
    "df = df[df.lang == \"en\"]\n",
    "#Trend_date is not necessary now\n",
    "df.drop([\"lang\",\"trend_date\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "exclude = '[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "non_ascii = re.compile(r'[^\\x00-\\x7F]+')\n",
    "#p.set_options(p.OPT.URL, p.OPT.EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trend'] = df['trend'].map(lambda x : x.lower())\n",
    "df['trend'] = df['trend'].map(lambda x : x.translate(remove_digits))\n",
    "df['trend'] = df['trend'].map(lambda x : re.sub(str(exclude), '', x))    \n",
    "\n",
    "\n",
    "df['text'] = df['text'].map(lambda x : x.lower())\n",
    "df['text'] = df['text'].map(lambda x : clean(x))\n",
    "df['text'] = df['text'].map(lambda x : x.translate(remove_digits))\n",
    "df['text'] = df['text'].map(lambda x : re.sub(str(exclude), '', x))    \n",
    "df['text'] = df['text'].map(lambda x : re.sub(non_ascii, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trend</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abel</td>\n",
       "      <td>rt  abel and rihanna are doing everything but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adheera</td>\n",
       "      <td>rt  unveiling from on july th,rt  hes back fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adlive</td>\n",
       "      <td>rt  adlive zero cast dates and locations annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aflbluescrows</td>\n",
       "      <td>night time is the right time to buy a for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afleaglesnorth</td>\n",
       "      <td>rt  problems for the cowboys already ezekiel e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>바이나인디어나인의우주</td>\n",
       "      <td>rt  we by chart is here to help the trainees i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>세훈찬열부르면돼</td>\n",
       "      <td>rt  sehun was the one who reached his hand out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>우리만믿어엑스원</td>\n",
       "      <td>rt  fighting for your debut,rt  cant wait for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>유벤투스</td>\n",
       "      <td>rt  camera founder  found hidden camera now yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>이닉알면오래된트친</td>\n",
       "      <td>boss,rt  camera founder  found hidden camera n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              trend                                               text\n",
       "0              abel  rt  abel and rihanna are doing everything but ...\n",
       "1           adheera  rt  unveiling from on july th,rt  hes back fro...\n",
       "2            adlive  rt  adlive zero cast dates and locations annou...\n",
       "3     aflbluescrows  night time is the right time to buy a for the ...\n",
       "4    afleaglesnorth  rt  problems for the cowboys already ezekiel e...\n",
       "..              ...                                                ...\n",
       "617     바이나인디어나인의우주  rt  we by chart is here to help the trainees i...\n",
       "618        세훈찬열부르면돼  rt  sehun was the one who reached his hand out...\n",
       "619        우리만믿어엑스원  rt  fighting for your debut,rt  cant wait for ...\n",
       "620            유벤투스  rt  camera founder  found hidden camera now yo...\n",
       "621       이닉알면오래된트친  boss,rt  camera founder  found hidden camera n...\n",
       "\n",
       "[622 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.groupby(['trend'])['text']\\\n",
    "            .apply(lambda x: ','.join(x)).reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt  adlive zero cast dates and locations announced for this years edition'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2,:]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [rt, abel, and, rihanna, are, doing, everythin...\n",
       "1      [rt, unveiling, from, on, july, th, ,, rt, hes...\n",
       "2      [rt, adlive, zero, cast, dates, and, locations...\n",
       "3      [night, time, is, the, right, time, to, buy, a...\n",
       "4      [rt, problems, for, the, cowboys, already, eze...\n",
       "                             ...                        \n",
       "617    [rt, we, by, chart, is, here, to, help, the, t...\n",
       "618    [rt, sehun, was, the, one, who, reached, his, ...\n",
       "619    [rt, fighting, for, your, debut, ,, rt, cant, ...\n",
       "620    [rt, camera, founder, found, hidden, camera, n...\n",
       "621    [boss, ,, rt, camera, founder, found, hidden, ...\n",
       "Length: 622, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "tokenized_df =  df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [abel, rihanna, do, everyth, drop, album, abel...\n",
       "1      [unveil, from, juli, back, from, dead, unveil,...\n",
       "2      [adliv, zero, cast, date, locat, announc, thi,...\n",
       "3      [night, time, right, time, celebr, here, somet...\n",
       "4      [problem, cowboy, alreadi, ezekiel, elliott, r...\n",
       "                             ...                        \n",
       "617    [chart, here, help, traine, achiev, their, dre...\n",
       "618    [sehun, reach, hand, love, love, sehun, reach,...\n",
       "619    [fight, your, debut, cant, wait, your, redebut...\n",
       "620    [camera, founder, found, hidden, camera, easil...\n",
       "621    [boss, camera, founder, found, hidden, camera,...\n",
       "Length: 622, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semmatize_text(text):\n",
    "    return [ps.stem(w)  for w in text if len(w)>3]\n",
    "ps = PorterStemmer() \n",
    "stemmed_dataset = tokenized_df.apply(semmatize_text)\n",
    "stemmed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_words=''.join(list(str(stemmed_dataset.values)))\n",
    "wordcloud = WordCloud(width = 800, height = 500, \n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate(dataset_words) \n",
    "\n",
    "# plt.figure(figsize = (5, 5), facecolor = None) \n",
    "# plt.imshow(wordcloud) \n",
    "# plt.axis(\"off\") \n",
    "# plt.tight_layout(pad = 0) \n",
    "  \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25155\n"
     ]
    }
   ],
   "source": [
    "dictionary_of_words = gensim.corpora.Dictionary(stemmed_dataset)\n",
    "print(len(dictionary_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abel 34\n",
      "advic 1\n",
      "aint 1\n",
      "album 25\n",
      "analysi 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'call'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_corpus = [dictionary_of_words.doc2bow(word) for word in stemmed_dataset]\n",
    "\n",
    "for corp in word_corpus[:1]:\n",
    "    for id, freq in corp[:5]:\n",
    "        print(dictionary_of_words[id],freq)\n",
    "\n",
    "dictionary_of_words[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Initial Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus=word_corpus,\n",
    "                                           id2word=dictionary_of_words,\n",
    "                                           num_topics=30, \n",
    "                                           random_state=101,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, topic in lda_model.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.5331569891775539\n"
     ]
    }
   ],
   "source": [
    "coherence_val = CoherenceModel(model=lda_model, texts=stemmed_dataset, \n",
    "                               dictionary=dictionary_of_words, coherence='c_v').get_coherence()\n",
    "print('Coherence Score: ', coherence_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Score is at num_topics=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics  5 coherence_value : 0.4322777314653199\n",
      "number of topics  10 coherence_value : 0.5775055519448571\n",
      "number of topics  15 coherence_value : 0.5608834659197979\n",
      "number of topics  20 coherence_value : 0.5734361394539241\n",
      "number of topics  25 coherence_value : 0.5684044645318396\n",
      "number of topics  30 coherence_value : 0.5331569891775539\n",
      "number of topics  35 coherence_value : 0.5607330212208015\n",
      "number of topics  40 coherence_value : 0.5323336344312884\n",
      "number of topics  45 coherence_value : 0.5174372823206751\n"
     ]
    }
   ],
   "source": [
    "lda_models=[]\n",
    "coherence_values = []\n",
    "for topic_number in range(5,50,5):\n",
    "    lda_model = models.ldamodel.LdaModel(corpus=word_corpus,\n",
    "                                           id2word=dictionary_of_words,\n",
    "                                           num_topics=topic_number, \n",
    "                                           random_state=101,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    lda_models.append(lda_model)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=stemmed_dataset, \n",
    "                                         dictionary=dictionary_of_words, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_values.append(coherence_lda)\n",
    "    print(\"number of topics \",topic_number,\"coherence_value :\" , coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with the best coherence_value\n",
    "lda_model_20 = models.ldamodel.LdaModel(corpus=word_corpus,\n",
    "                                           id2word=dictionary_of_words,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=1,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5945144461036688\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "cohr_val = CoherenceModel(model=lda_model_20, texts=stemmed_dataset, dictionary=dictionary_of_words, coherence='c_v').get_coherence()\n",
    "\n",
    "print('\\nCoherence Score: ', cohr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDAMulticore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_multicore_model = models.ldamulticore.LdaMulticore(corpus=word_corpus, \n",
    "                                                              num_topics=20, \n",
    "                                                              id2word=dictionary_of_words,                                                             \n",
    "                                                              chunksize=100, \n",
    "                                                              passes=50,                                \n",
    "                                                              alpha='symmetric',\n",
    "                                                              eta=0.1,\n",
    "                                                              decay=0.5, \n",
    "                                                              offset=1.0, \n",
    "                                                              gamma_threshold=0.001,\n",
    "                                                              random_state=101,\n",
    "                                                              minimum_probability=0.01,\n",
    "                                                              minimum_phi_value=0.01,\n",
    "                                                              per_word_topics=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.48426339501658616\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "cohr_lda_multicore_model1 = CoherenceModel(model=lda_multicore_model, texts=stemmed_dataset, \n",
    "                                           dictionary=dictionary_of_words, coherence='c_v').get_coherence()\n",
    "print('\\nCoherence Score: ', cohr_lda_multicore_model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "[(21, 0.2869622), (4, 0.28240708), (16, 0.105208606), (25, 0.089200296), (28, 0.03805037), (17, 0.033370774), (31, 0.0281642), (44, 0.01557884), (26, 0.012868903), (18, 0.011408153)]\n"
     ]
    }
   ],
   "source": [
    "v = lda_model[word_corpus[2]]\n",
    "print(type(lda_model[word_corpus[2]]))\n",
    "z=sorted(v[0], key=lambda tup: -1*tup[1])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.2822459),\n",
       " (16, 0.10521838),\n",
       " (17, 0.03337036),\n",
       " (18, 0.011408333),\n",
       " (21, 0.2871611),\n",
       " (25, 0.089150466),\n",
       " (26, 0.01286891),\n",
       " (28, 0.03805329),\n",
       " (31, 0.028163819),\n",
       " (44, 0.015578847)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[word_corpus[2]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.2864798605442047\t \n",
      "Topic: 0.040*\"follow\" + 0.033*\"enter\" + 0.032*\"thi\" + 0.027*\"music\" + 0.023*\"chanc\" + 0.020*\"your\" + 0.020*\"congratul\" + 0.018*\"villag\" + 0.016*\"with\" + 0.015*\"winner\"\n",
      "\n",
      "Score: 0.282893568277359\t \n",
      "Topic: 0.023*\"thi\" + 0.020*\"with\" + 0.012*\"have\" + 0.011*\"your\" + 0.010*\"more\" + 0.009*\"that\" + 0.009*\"will\" + 0.008*\"from\" + 0.008*\"time\" + 0.007*\"here\"\n",
      "\n",
      "Score: 0.10516836494207382\t \n",
      "Topic: 0.057*\"preview\" + 0.045*\"solo\" + 0.040*\"seoul\" + 0.039*\"mark\" + 0.038*\"fansign\" + 0.035*\"teaser\" + 0.031*\"event\" + 0.029*\"fancam\" + 0.025*\"tour\" + 0.024*\"with\"\n",
      "\n",
      "Score: 0.08927897363901138\t \n",
      "Topic: 0.109*\"cover\" + 0.067*\"edit\" + 0.047*\"champion\" + 0.047*\"from\" + 0.043*\"back\" + 0.043*\"girl\" + 0.042*\"small\" + 0.039*\"villag\" + 0.034*\"biggi\" + 0.032*\"come\"\n",
      "\n",
      "Score: 0.038033097982406616\t \n",
      "Topic: 0.015*\"that\" + 0.015*\"with\" + 0.012*\"trump\" + 0.012*\"from\" + 0.010*\"when\" + 0.009*\"about\" + 0.008*\"peopl\" + 0.008*\"thi\" + 0.008*\"will\" + 0.007*\"news\"\n",
      "\n",
      "Score: 0.03334563225507736\t \n",
      "Topic: 0.025*\"your\" + 0.024*\"that\" + 0.019*\"from\" + 0.018*\"what\" + 0.015*\"turn\" + 0.014*\"thi\" + 0.014*\"with\" + 0.013*\"kong\" + 0.013*\"hong\" + 0.012*\"just\"\n",
      "\n",
      "Score: 0.028164707124233246\t \n",
      "Topic: 0.030*\"will\" + 0.030*\"thi\" + 0.022*\"tweet\" + 0.016*\"let\" + 0.015*\"know\" + 0.015*\"have\" + 0.015*\"them\" + 0.014*\"dont\" + 0.014*\"about\" + 0.014*\"they\"\n",
      "\n",
      "Score: 0.015578808262944221\t \n",
      "Topic: 0.072*\"face\" + 0.024*\"your\" + 0.024*\"that\" + 0.020*\"like\" + 0.014*\"thi\" + 0.012*\"when\" + 0.011*\"just\" + 0.010*\"want\" + 0.009*\"look\" + 0.009*\"hype\"\n",
      "\n",
      "Score: 0.012868868187069893\t \n",
      "Topic: 0.057*\"love\" + 0.041*\"your\" + 0.038*\"thi\" + 0.033*\"thank\" + 0.033*\"song\" + 0.019*\"with\" + 0.019*\"best\" + 0.016*\"part\" + 0.014*\"male\" + 0.013*\"artist\"\n",
      "\n",
      "Score: 0.011407549493014812\t \n",
      "Topic: 0.054*\"famou\" + 0.035*\"more\" + 0.035*\"info\" + 0.028*\"notic\" + 0.028*\"open\" + 0.027*\"ticket\" + 0.027*\"seven\" + 0.020*\"that\" + 0.020*\"secret\" + 0.016*\"korean\"\n"
     ]
    }
   ],
   "source": [
    "for  index,score in sorted(lda_model[word_corpus[2]][0], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
