{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "import gensim\n",
    "\n",
    "\n",
    "import preprocessor as p\n",
    "from preprocessor.api import clean\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: before saving the script try cleaning with RT @... and other punctiation stuff \n",
    "# may be # can stay it's a bit problematic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-07-26_trends.csv',\n",
       " '2019-07-18_trends.csv',\n",
       " '2019-07-20_trends.csv',\n",
       " '2019-07-07_trends.csv',\n",
       " '2019-07-12_trends.csv']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../Data\"\n",
    "TWEETS_PATH = os.path.join(DATA_DIR, 'tweets')\n",
    "TREND_PATH = os.path.join(DATA_DIR, 'trends')\n",
    "SAVE_PATH = os.path.join(DATA_DIR, 'save')\n",
    "os.listdir(SAVE_PATH)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( os.path.join(SAVE_PATH,\"2019-07-26_trends.csv\"),\n",
    "                header=0, usecols=[4,5,6,7], parse_dates=['trend_date'])\n",
    "#Take only english ones\n",
    "df = df[df.lang == \"en\"]\n",
    "#Trend_date is not necessary now\n",
    "df.drop([\"lang\",\"trend_date\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "exclude = '[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "non_ascii = re.compile(r'[^\\x00-\\x7F]+')\n",
    "#p.set_options(p.OPT.URL, p.OPT.EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trend'] = df['trend'].map(lambda x : x.lower())\n",
    "df['trend'] = df['trend'].map(lambda x : x.translate(remove_digits))\n",
    "df['trend'] = df['trend'].map(lambda x : re.sub(str(exclude), '', x))    \n",
    "\n",
    "\n",
    "df['text'] = df['text'].map(lambda x : x.lower())\n",
    "df['text'] = df['text'].map(lambda x : clean(x))\n",
    "df['text'] = df['text'].map(lambda x : x.translate(remove_digits))\n",
    "df['text'] = df['text'].map(lambda x : re.sub(str(exclude), '', x))    \n",
    "df['text'] = df['text'].map(lambda x : re.sub(non_ascii, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trend</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abel</td>\n",
       "      <td>rt  abel and rihanna are doing everything but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adheera</td>\n",
       "      <td>rt  unveiling from on july th,rt  hes back fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adlive</td>\n",
       "      <td>rt  adlive zero cast dates and locations annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aflbluescrows</td>\n",
       "      <td>night time is the right time to buy a for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afleaglesnorth</td>\n",
       "      <td>rt  problems for the cowboys already ezekiel e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>바이나인디어나인의우주</td>\n",
       "      <td>rt  we by chart is here to help the trainees i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>세훈찬열부르면돼</td>\n",
       "      <td>rt  sehun was the one who reached his hand out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>우리만믿어엑스원</td>\n",
       "      <td>rt  fighting for your debut,rt  cant wait for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>유벤투스</td>\n",
       "      <td>rt  camera founder  found hidden camera now yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>이닉알면오래된트친</td>\n",
       "      <td>boss,rt  camera founder  found hidden camera n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              trend                                               text\n",
       "0              abel  rt  abel and rihanna are doing everything but ...\n",
       "1           adheera  rt  unveiling from on july th,rt  hes back fro...\n",
       "2            adlive  rt  adlive zero cast dates and locations annou...\n",
       "3     aflbluescrows  night time is the right time to buy a for the ...\n",
       "4    afleaglesnorth  rt  problems for the cowboys already ezekiel e...\n",
       "..              ...                                                ...\n",
       "617     바이나인디어나인의우주  rt  we by chart is here to help the trainees i...\n",
       "618        세훈찬열부르면돼  rt  sehun was the one who reached his hand out...\n",
       "619        우리만믿어엑스원  rt  fighting for your debut,rt  cant wait for ...\n",
       "620            유벤투스  rt  camera founder  found hidden camera now yo...\n",
       "621       이닉알면오래된트친  boss,rt  camera founder  found hidden camera n...\n",
       "\n",
       "[622 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.groupby(['trend'])['text']\\\n",
    "            .apply(lambda x: ','.join(x)).reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt  adlive zero cast dates and locations announced for this years edition'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2,:]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [rt, abel, and, rihanna, are, doing, everythin...\n",
       "1      [rt, unveiling, from, on, july, th, ,, rt, hes...\n",
       "2      [rt, adlive, zero, cast, dates, and, locations...\n",
       "3      [night, time, is, the, right, time, to, buy, a...\n",
       "4      [rt, problems, for, the, cowboys, already, eze...\n",
       "                             ...                        \n",
       "617    [rt, we, by, chart, is, here, to, help, the, t...\n",
       "618    [rt, sehun, was, the, one, who, reached, his, ...\n",
       "619    [rt, fighting, for, your, debut, ,, rt, cant, ...\n",
       "620    [rt, camera, founder, found, hidden, camera, n...\n",
       "621    [boss, ,, rt, camera, founder, found, hidden, ...\n",
       "Length: 622, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "tokenized_df =  df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [abel, rihanna, do, everyth, drop, album, abel...\n",
       "1      [unveil, from, juli, back, from, dead, unveil,...\n",
       "2      [adliv, zero, cast, date, locat, announc, thi,...\n",
       "3      [night, time, right, time, celebr, here, somet...\n",
       "4      [problem, cowboy, alreadi, ezekiel, elliott, r...\n",
       "                             ...                        \n",
       "617    [chart, here, help, traine, achiev, their, dre...\n",
       "618    [sehun, reach, hand, love, love, sehun, reach,...\n",
       "619    [fight, your, debut, cant, wait, your, redebut...\n",
       "620    [camera, founder, found, hidden, camera, easil...\n",
       "621    [boss, camera, founder, found, hidden, camera,...\n",
       "Length: 622, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semmatize_text(text):\n",
    "    return [ps.stem(w)  for w in text if len(w)>3]\n",
    "ps = PorterStemmer() \n",
    "stemmed_dataset = tokenized_df.apply(semmatize_text)\n",
    "stemmed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_words=''.join(list(str(stemmed_dataset.values)))\n",
    "wordcloud = WordCloud(width = 800, height = 500, \n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate(dataset_words) \n",
    "\n",
    "# plt.figure(figsize = (5, 5), facecolor = None) \n",
    "# plt.imshow(wordcloud) \n",
    "# plt.axis(\"off\") \n",
    "# plt.tight_layout(pad = 0) \n",
    "  \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25155\n"
     ]
    }
   ],
   "source": [
    "dictionary_of_words = gensim.corpora.Dictionary(stemmed_dataset)\n",
    "print(len(dictionary_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abel 34\n",
      "advic 1\n",
      "aint 1\n",
      "album 25\n",
      "analysi 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'call'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_corpus = [dictionary_of_words.doc2bow(word) for word in stemmed_dataset]\n",
    "\n",
    "for corp in word_corpus[:1]:\n",
    "    for id, freq in corp[:5]:\n",
    "        print(dictionary_of_words[id],freq)\n",
    "\n",
    "dictionary_of_words[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Initial Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus=word_corpus,\n",
    "                                           id2word=dictionary_of_words,\n",
    "                                           num_topics=30, \n",
    "                                           random_state=101,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, topic in lda_model.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.5331569891775539\n"
     ]
    }
   ],
   "source": [
    "coherence_val = CoherenceModel(model=lda_model, texts=stemmed_dataset, \n",
    "                               dictionary=dictionary_of_words, coherence='c_v').get_coherence()\n",
    "print('Coherence Score: ', coherence_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Score is at num_topics=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics  5 coherence_value : 0.4322777314653199\n",
      "number of topics  10 coherence_value : 0.5775055519448571\n",
      "number of topics  15 coherence_value : 0.5608834659197979\n",
      "number of topics  20 coherence_value : 0.5734361394539241\n",
      "number of topics  25 coherence_value : 0.5684044645318396\n",
      "number of topics  30 coherence_value : 0.5331569891775539\n",
      "number of topics  35 coherence_value : 0.5607330212208015\n",
      "number of topics  40 coherence_value : 0.5323336344312884\n",
      "number of topics  45 coherence_value : 0.5174372823206751\n"
     ]
    }
   ],
   "source": [
    "lda_models=[]\n",
    "coherence_values = []\n",
    "for topic_number in range(5,50,5):\n",
    "    lda_model = models.ldamodel.LdaModel(corpus=word_corpus,\n",
    "                                           id2word=dictionary_of_words,\n",
    "                                           num_topics=topic_number, \n",
    "                                           random_state=101,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    lda_models.append(lda_model)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=stemmed_dataset, \n",
    "                                         dictionary=dictionary_of_words, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_values.append(coherence_lda)\n",
    "    print(\"number of topics \",topic_number,\"coherence_value :\" , coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with the best coherence_value\n",
    "lda_model_20 = models.ldamodel.LdaModel(corpus=word_corpus,\n",
    "                                           id2word=dictionary_of_words,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=1,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5945144461036688\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "cohr_val = CoherenceModel(model=lda_model_20, texts=stemmed_dataset, dictionary=dictionary_of_words, coherence='c_v').get_coherence()\n",
    "\n",
    "print('\\nCoherence Score: ', cohr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDAMulticore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_multicore_model = models.ldamulticore.LdaMulticore(corpus=word_corpus, \n",
    "                                                              num_topics=20, \n",
    "                                                              id2word=dictionary_of_words,                                                             \n",
    "                                                              chunksize=100, \n",
    "                                                              passes=50,                                \n",
    "                                                              alpha='symmetric',\n",
    "                                                              eta=0.1,\n",
    "                                                              decay=0.5, \n",
    "                                                              offset=1.0, \n",
    "                                                              gamma_threshold=0.001,\n",
    "                                                              random_state=101,\n",
    "                                                              minimum_probability=0.01,\n",
    "                                                              minimum_phi_value=0.01,\n",
    "                                                              per_word_topics=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.48426339501658616\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "cohr_lda_multicore_model1 = CoherenceModel(model=lda_multicore_model, texts=stemmed_dataset, \n",
    "                                           dictionary=dictionary_of_words, coherence='c_v').get_coherence()\n",
    "print('\\nCoherence Score: ', cohr_lda_multicore_model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "[(15, 0.44074607), (4, 0.3553429), (11, 0.12210046), (3, 0.011222988), (0, 0.010196379)]\n"
     ]
    }
   ],
   "source": [
    "v = lda_model_20[word_corpus[2]]\n",
    "print(type(lda_model[word_corpus[2]]))\n",
    "z=sorted(v[0], key=lambda tup: -1*tup[1])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.7901357),\n",
       " (6, 0.10349235),\n",
       " (11, 0.021349145),\n",
       " (13, 0.05406458),\n",
       " (14, 0.019050092)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_20[word_corpus[500]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4408090114593506\t \n",
      "Topic: 0.040*\"mitch\" + 0.039*\"elect\" + 0.031*\"mcconnel\" + 0.029*\"russian\" + 0.025*\"vote\" + 0.020*\"traitor\" + 0.020*\"refus\" + 0.018*\"secur\" + 0.018*\"russia\" + 0.017*\"democraci\"\n",
      "\n",
      "Score: 0.3552166521549225\t \n",
      "Topic: 0.023*\"thi\" + 0.020*\"with\" + 0.012*\"have\" + 0.011*\"your\" + 0.010*\"more\" + 0.009*\"that\" + 0.009*\"will\" + 0.008*\"from\" + 0.008*\"time\" + 0.007*\"here\"\n",
      "\n",
      "Score: 0.1221638098359108\t \n",
      "Topic: 0.054*\"regim\" + 0.028*\"rihanna\" + 0.025*\"abel\" + 0.024*\"everyth\" + 0.022*\"ann\" + 0.021*\"do\" + 0.021*\"drop\" + 0.021*\"mike\" + 0.014*\"hathaway\" + 0.013*\"essenc\"\n",
      "\n",
      "Score: 0.011222981847822666\t \n",
      "Topic: 0.037*\"morn\" + 0.033*\"sunday\" + 0.020*\"atlant\" + 0.009*\"extra\" + 0.005*\"endlich\" + 0.005*\"allein\" + 0.004*\"queremo\" + 0.004*\"ampkendricklamar\" + 0.003*\"armin\" + 0.003*\"kygo\"\n",
      "\n",
      "Score: 0.010196379385888577\t \n",
      "Topic: 0.049*\"bernal\" + 0.036*\"egan\" + 0.034*\"stage\" + 0.034*\"first\" + 0.034*\"yellow\" + 0.032*\"jersey\" + 0.030*\"pinot\" + 0.025*\"best\" + 0.023*\"tour\" + 0.022*\"franc\"\n"
     ]
    }
   ],
   "source": [
    "for  index,score in sorted(lda_model_20[word_corpus[2]][0], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = lda_model_20.show_topics()\n",
    "topics[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  '0.023*\"thi\" + 0.017*\"that\" + 0.015*\"with\" + 0.015*\"your\" + 0.012*\"have\" + 0.009*\"will\" + 0.008*\"from\" + 0.008*\"like\" + 0.007*\"love\" + 0.007*\"about\"')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_20.print_topics(1) # list of tuples where tuple[0] = topicID tuple[1] = topicWords\n",
    "# what is written inside is index \n",
    "# in the list they are ordered but not 1,2,3 I guess based on size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.097*\"face\" + 0.023*\"famou\" + 0.018*\"that\" + 0.013*\"your\" + 0.013*\"when\" + 0.012*\"like\" + 0.012*\"look\" + 0.010*\"they\" + 0.009*\"aid\" + 0.009*\"trump\"\n",
      "Topic: 1 \n",
      "Words: 0.000*\"hughbrownston\" + 0.000*\"hyphynhumbl\" + 0.000*\"hillaryclitori\" + 0.000*\"honeybunnyb\" + 0.000*\"honmychest\" + 0.000*\"hellosh\" + 0.000*\"itslexxiebabi\" + 0.000*\"isaiahheath\" + 0.000*\"heddicundl\" + 0.000*\"illswagg\"\n",
      "Topic: 2 \n",
      "Words: 0.126*\"lucki\" + 0.067*\"brown\" + 0.065*\"chri\" + 0.042*\"video\" + 0.041*\"guidanc\" + 0.032*\"drake\" + 0.018*\"again\" + 0.017*\"see\" + 0.017*\"front\" + 0.016*\"march\"\n",
      "Topic: 3 \n",
      "Words: 0.046*\"what\" + 0.046*\"fuck\" + 0.042*\"scene\" + 0.027*\"that\" + 0.026*\"toni\" + 0.026*\"cover\" + 0.026*\"villag\" + 0.025*\"thi\" + 0.023*\"show\" + 0.022*\"they\"\n",
      "Topic: 4 \n",
      "Words: 0.023*\"thi\" + 0.017*\"that\" + 0.015*\"with\" + 0.015*\"your\" + 0.012*\"have\" + 0.009*\"will\" + 0.008*\"from\" + 0.008*\"like\" + 0.007*\"love\" + 0.007*\"about\"\n",
      "Topic: 5 \n",
      "Words: 0.069*\"madrid\" + 0.061*\"real\" + 0.019*\"goal\" + 0.018*\"costa\" + 0.018*\"atletico\" + 0.017*\"diego\" + 0.014*\"score\" + 0.013*\"against\" + 0.011*\"game\" + 0.011*\"match\"\n",
      "Topic: 6 \n",
      "Words: 0.056*\"town\" + 0.056*\"road\" + 0.035*\"seoul\" + 0.024*\"with\" + 0.023*\"chanc\" + 0.021*\"nicki\" + 0.021*\"remix\" + 0.018*\"album\" + 0.018*\"rapper\" + 0.016*\"singl\"\n",
      "Topic: 7 \n",
      "Words: 0.057*\"kolasinac\" + 0.035*\"ozil\" + 0.031*\"sead\" + 0.029*\"mesut\" + 0.029*\"arsen\" + 0.025*\"with\" + 0.023*\"attack\" + 0.019*\"thi\" + 0.018*\"fight\" + 0.013*\"both\"\n",
      "Topic: 8 \n",
      "Words: 0.035*\"okoth\" + 0.030*\"nine\" + 0.023*\"pinot\" + 0.021*\"girl\" + 0.020*\"bori\" + 0.017*\"biggi\" + 0.017*\"johnson\" + 0.017*\"from\" + 0.016*\"english\" + 0.016*\"london\"\n",
      "Topic: 9 \n",
      "Words: 0.033*\"mitch\" + 0.032*\"elect\" + 0.026*\"mcconnel\" + 0.024*\"russian\" + 0.021*\"vote\" + 0.017*\"trump\" + 0.017*\"traitor\" + 0.016*\"refus\" + 0.015*\"secur\" + 0.015*\"russia\"\n",
      "Topic: 10 \n",
      "Words: 0.092*\"sweden\" + 0.070*\"american\" + 0.057*\"be\" + 0.047*\"african\" + 0.043*\"commun\" + 0.036*\"unit\" + 0.034*\"state\" + 0.031*\"minist\" + 0.030*\"rocki\" + 0.030*\"watch\"\n",
      "Topic: 11 \n",
      "Words: 0.018*\"your\" + 0.018*\"debut\" + 0.015*\"thi\" + 0.014*\"with\" + 0.014*\"love\" + 0.012*\"daniel\" + 0.011*\"that\" + 0.011*\"trend\" + 0.010*\"what\" + 0.009*\"like\"\n",
      "Topic: 12 \n",
      "Words: 0.070*\"stronger\" + 0.061*\"tweet\" + 0.060*\"melon\" + 0.050*\"geni\" + 0.041*\"bug\" + 0.038*\"from\" + 0.031*\"juli\" + 0.027*\"tini\" + 0.026*\"pelo\" + 0.025*\"first\"\n",
      "Topic: 13 \n",
      "Words: 0.261*\"album\" + 0.232*\"releas\" + 0.230*\"dream\" + 0.028*\"mini\" + 0.023*\"boom\" + 0.022*\"music\" + 0.016*\"itun\" + 0.011*\"view\" + 0.011*\"download\" + 0.009*\"appl\"\n",
      "Topic: 14 \n",
      "Words: 0.028*\"brave\" + 0.022*\"kargil\" + 0.018*\"sacrific\" + 0.017*\"soldier\" + 0.016*\"india\" + 0.016*\"nation\" + 0.016*\"women\" + 0.016*\"salut\" + 0.012*\"celebr\" + 0.011*\"with\"\n",
      "Topic: 15 \n",
      "Words: 0.085*\"chanc\" + 0.033*\"follow\" + 0.025*\"thi\" + 0.023*\"your\" + 0.017*\"enter\" + 0.017*\"retweet\" + 0.016*\"give\" + 0.015*\"with\" + 0.014*\"winner\" + 0.010*\"giveaway\"\n",
      "Topic: 16 \n",
      "Words: 0.067*\"ikon\" + 0.047*\"will\" + 0.033*\"hanbin\" + 0.028*\"today\" + 0.022*\"thi\" + 0.022*\"hashtag\" + 0.020*\"fight\" + 0.019*\"them\" + 0.019*\"tweet\" + 0.018*\"well\"\n",
      "Topic: 17 \n",
      "Words: 0.083*\"accomplic\" + 0.037*\"that\" + 0.036*\"enabl\" + 0.032*\"bett\" + 0.028*\"come\" + 0.028*\"block\" + 0.024*\"legisl\" + 0.022*\"system\" + 0.022*\"call\" + 0.021*\"would\"\n",
      "Topic: 18 \n",
      "Words: 0.069*\"rocki\" + 0.046*\"focu\" + 0.045*\"freedom\" + 0.045*\"give\" + 0.045*\"doesnt\" + 0.043*\"around\" + 0.042*\"work\" + 0.042*\"crime\" + 0.042*\"should\" + 0.042*\"other\"\n",
      "Topic: 19 \n",
      "Words: 0.114*\"flash\" + 0.077*\"pose\" + 0.028*\"video\" + 0.027*\"love\" + 0.019*\"song\" + 0.014*\"peak\" + 0.011*\"watch\" + 0.011*\"music\" + 0.010*\"offici\" + 0.009*\"featur\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_20.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Trend-Key_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for key-words finding\n",
    "lda_model_key_words = models.ldamodel.LdaModel(corpus=word_corpus,\n",
    "                                           id2word=dictionary_of_words,\n",
    "                                           num_topics=600, \n",
    "                                           random_state=1,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.48426339501658616\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "cohr_lda_multicore_model1 = CoherenceModel(model=lda_multicore_model, texts=stemmed_dataset, \n",
    "                                           dictionary=dictionary_of_words, coherence='c_v').get_coherence()\n",
    "print('\\nCoherence Score: ', cohr_lda_multicore_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(242,\n",
       "  '0.000*\"hughbrownston\" + 0.000*\"hyphynhumbl\" + 0.000*\"hillaryclitori\" + 0.000*\"honeybunnyb\" + 0.000*\"honmychest\" + 0.000*\"hellosh\" + 0.000*\"itslexxiebabi\" + 0.000*\"isaiahheath\" + 0.000*\"heddicundl\" + 0.000*\"illswagg\"'),\n",
       " (54,\n",
       "  '0.000*\"hughbrownston\" + 0.000*\"hyphynhumbl\" + 0.000*\"hillaryclitori\" + 0.000*\"honeybunnyb\" + 0.000*\"honmychest\" + 0.000*\"hellosh\" + 0.000*\"itslexxiebabi\" + 0.000*\"isaiahheath\" + 0.000*\"heddicundl\" + 0.000*\"illswagg\"'),\n",
       " (291,\n",
       "  '0.059*\"mitch\" + 0.050*\"mcconnel\" + 0.034*\"seoul\" + 0.030*\"traitor\" + 0.029*\"refus\" + 0.015*\"biggest\" + 0.015*\"remix\" + 0.015*\"singapor\" + 0.013*\"that\" + 0.013*\"moscow\"'),\n",
       " (415,\n",
       "  '0.029*\"thi\" + 0.019*\"will\" + 0.015*\"pleas\" + 0.014*\"let\" + 0.013*\"know\" + 0.013*\"them\" + 0.013*\"love\" + 0.013*\"that\" + 0.012*\"with\" + 0.012*\"well\"'),\n",
       " (151,\n",
       "  '0.024*\"that\" + 0.020*\"thi\" + 0.020*\"have\" + 0.017*\"with\" + 0.016*\"your\" + 0.010*\"from\" + 0.009*\"will\" + 0.008*\"dont\" + 0.008*\"want\" + 0.008*\"make\"')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_key_words.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
