{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary, datapath\n",
    "import gensim\n",
    "\n",
    "\n",
    "import preprocessor as p\n",
    "from preprocessor.api import clean\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semmatize_text(w):\n",
    "    ps = PorterStemmer() \n",
    "    return ps.stem(w)\n",
    "stop_words_stem = [semmatize_text(x) for x in stop_words]\n",
    "stop_words.extend(stop_words_stem)\n",
    "stop_words = list(dict.fromkeys(stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(stop_words)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../Data\"\n",
    "TWEETS_PATH = os.path.join(DATA_DIR, 'tweets')\n",
    "TREND_PATH = os.path.join(DATA_DIR, 'trends')\n",
    "SAVE_PATH = os.path.join(DATA_DIR, 'save')\n",
    "STATS_PATH = os.path.join(DATA_DIR, 'stats')\n",
    "TOPICS_PATH = os.path.join(DATA_DIR, 'topics')\n",
    "\n",
    "os.listdir(SAVE_PATH)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train =  pd.read_csv(os.path.join(SAVE_PATH, \"lda_train_data\"), header=0, parse_dates=['trend_date'])\n",
    "dfs_test =  pd.read_csv(os.path.join(SAVE_PATH, \"lda_test_data\"), header=0, parse_dates=['trend_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs_train.shape)\n",
    "dfs_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs_test.shape)\n",
    "dfs_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semmatize_text(text):\n",
    "    ps = PorterStemmer() \n",
    "    return [ps.stem(w)  for w in text if len(w)>3]\n",
    "\n",
    "def tokanize_text(trend_doc):\n",
    "    return trend_doc.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [word for word in texts if word not in stop_words ]\n",
    "\n",
    "def process_lda_format(trend_doc):\n",
    "    tokenized_df = tokanize_text(trend_doc)\n",
    "    stemmed_dataset = tokenized_df.apply(semmatize_text)\n",
    "    stemmed_dataset = stemmed_dataset.map(lambda x: remove_stopwords(x))\n",
    "    return stemmed_dataset\n",
    "\n",
    "def initialize_corpus_and_dictionary(stemmed_dataset):\n",
    "    dictionary_of_words = gensim.corpora.Dictionary(stemmed_dataset)\n",
    "    word_corpus = [dictionary_of_words.doc2bow(word) for word in stemmed_dataset]\n",
    "    \n",
    "    return word_corpus, dictionary_of_words\n",
    "\n",
    "def lda_datasets(trend_doc):\n",
    "    stemmed_dataset = process_lda_format(trend_doc)\n",
    "    corpus, dictionary = initialize_corpus_and_dictionary(stemmed_dataset)\n",
    "    \n",
    "    return stemmed_dataset, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lda():\n",
    "    # Model with the best coherence_value\n",
    "    lda_model_16 = models.ldamodel.LdaModel(corpus = corpus, id2word=dictionary, num_topics=16, \n",
    "                                            random_state=1, update_every=1, chunksize=100,\n",
    "                                            passes=50, alpha='auto', per_word_topics=True)\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    temp_file = datapath(os.path.join(cwd, \"models/lda_model_16\"))\n",
    "    print('Model is saving... at', temp_file)\n",
    "    lda_model_16.save(temp_file)\n",
    "    \n",
    "    # Compute Perplexity Score\n",
    "    print('Perplexity Score: ', lda_model_16.log_perplexity(corpus)) \n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    cohr_val = CoherenceModel(model=lda_model_16, texts=stemmed_dataset, dictionary=dictionary,\n",
    "                                      coherence='c_v').get_coherence()\n",
    "\n",
    "    print('Coherence Score: ', cohr_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsLDA = dfs_train.loc[:,[\"trend\",\"text\"]]\n",
    "dfsLDA.dropna(inplace=True)\n",
    "trend_doc = dfsLDA.groupby(['trend'])['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "trend_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_dataset, corpus, dictionary = lda_datasets(trend_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "temp_file = datapath(os.path.join(cwd, \"models/lda_model_16\"))\n",
    "temp_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a potentially pretrained model from disk.\n",
    "lda = models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_match = re.compile(r'\\\"\\w+\\\"')\n",
    "\n",
    "for idx, topic in lda.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "#     topic_file = open(os.path.join(TOPICS_PATH, \"topic-\"+str(idx)+\".txt\"), \"w+\")\n",
    "#     words = re.findall( words_match, topic)\n",
    "#     topic_file.write( str(words) )\n",
    "#     topic_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_doc = pd.read_csv(os.path.join(DATA_DIR, 'categories'), header=0)\n",
    "target_doc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CHECK WHAT PERCENTAGE OF THE TWEETS ARE MATCH\n",
    "\n",
    "# df_ana = pd.read_csv(os.path.join(TWEETS_PATH,\"2019-09-01_tweetsevenmorebasic.csv.bz2.bz2\"), header=0)\n",
    "# df_ana.shape\n",
    "# print(df.shape[0]/df_ana.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
