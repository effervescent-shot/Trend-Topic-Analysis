{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-08-31_tweetsevenmorebasic.csv',\n",
       " 'categories',\n",
       " 'tweets',\n",
       " 'topics',\n",
       " 'save',\n",
       " 'stats',\n",
       " 'trends']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../Data\"\n",
    "TWEETS_PATH = os.path.join(DATA_DIR, 'tweets')\n",
    "TREND_PATH = os.path.join(DATA_DIR, 'trends')\n",
    "SAVE_PATH = os.path.join(DATA_DIR, 'save')\n",
    "STATS_PATH = os.path.join(DATA_DIR, 'stats')\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "RuleMatch = namedtuple('RuleMatch', 'trend rule')\n",
    "RuleMatchStats = namedtuple('RuleMatchStats', 'tweetSize matchedSize')\n",
    "TrendMatch = namedtuple('TrendMatch', 'trend match matchRule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ezgi', 'nihal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'EZGI#NIHAL'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = RuleMatch(trend='nihal', rule='simple' )\n",
    "r2 = RuleMatch(trend='ezgi', rule='simple' )\n",
    "ll = set([r1, r2])\n",
    "\n",
    "\n",
    "rm = RuleMatch(*zip(*ll))\n",
    "rl = list(rm.trend)\n",
    "print(rl)\n",
    "'#'.join(rl).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(trend_topic):\n",
    "    \"\"\"\n",
    "    param: trend_topic is a single word trend topic \n",
    "\n",
    "    Check if a trend word is in the form of camel case without # \n",
    "    If so split the camel case to its words\n",
    "    \n",
    "    return: list of all words in camel case format \n",
    "    \"\"\" \n",
    "    trend_topic = re.sub('#','', trend_topic)\n",
    "    match_list = []\n",
    "#     for identifier in trend_topic:    \n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', trend_topic)\n",
    "    match_list += [m.group(0) for m in matches]\n",
    "    \n",
    "    if(len(match_list))==1:\n",
    "        return []\n",
    "    return match_list\n",
    "        \n",
    "def onegram_augment(trend_topic):\n",
    "    \"\"\"\n",
    "    param: trend_topic is a single word trend topic\n",
    "\n",
    "    From a trend in onegram set create augmented set of the trend by\n",
    "    apply upper-lower case transformation\n",
    "    split the hash and rejoin\n",
    "    split the camel case and rejoin\n",
    "    write a rule for every augmentation\n",
    "\n",
    "    return: set of augmented-trend-topic, set of augmeted-trend-topic if it was also camelcase\n",
    "    \"\"\"\n",
    "\n",
    "    onegram = set([RuleMatch(trend=trend_topic, rule='simple')])\n",
    "    onegram_up = set([RuleMatch(trend=gram.trend.upper(), rule='simple-upper') for gram in onegram])\n",
    "    onegram_lower = set([RuleMatch(trend=gram.trend.lower(), rule='simple-lower') for gram in onegram])\n",
    "    \n",
    "    nohash = set()\n",
    "    nohash_up = set()\n",
    "    nohash_lower = set()\n",
    "    \n",
    "    if '#' in trend_topic:\n",
    "        nohash = set([RuleMatch(trend=re.sub('#', '', trend_topic), rule='no-hashtag')])\n",
    "        nohash_up = set([RuleMatch(trend=gram.trend.upper(), rule='no-hashtag-upper') for gram in nohash])\n",
    "        nohash_lower = set([RuleMatch(trend=gram.trend.lower(), rule='no-hashtag-lower') for gram in nohash])\n",
    "\n",
    "    camelCase = camel_case_split(trend_topic)\n",
    "    camelSplit = set()\n",
    "    if len(camelCase) != 0:\n",
    "        cc = [RuleMatch(trend=gram, rule='camel') for gram in camelCase]\n",
    "        cc_up = [RuleMatch(trend=gram.upper(), rule='camel-upper') for gram in camelCase]\n",
    "        cc_lower = [RuleMatch(trend=gram.lower(), rule='camel-lower') for gram in camelCase]\n",
    "\n",
    "        ccHashed = set([RuleMatch(trend='#' + gram, rule='camel-hashtag') for gram in camelCase])\n",
    "        ccHashed_up = set([RuleMatch(trend='#' + gram.trend, rule='camel-upper-hashtag') for gram in cc_up])\n",
    "        ccHashed_lower = set([RuleMatch(trend='#' + gram.trend, rule='camel-lower-hashtag') for gram in cc_lower])\n",
    "\n",
    "        ccHashJoined = set([RuleMatch(trend='#'.join(camelCase), rule='camel-join-hashtag')])\n",
    "        ccHashJoined_up = set([RuleMatch(trend='#'.join(camelCase).upper(), rule='camel-upper-join-hashtag')])\n",
    "        ccHashJoined_lower = set([RuleMatch(trend='#'.join(camelCase).lower(), rule='camel-lower-join-hashtag')])\n",
    "\n",
    "        \"\"\"\n",
    "        This part is to add it later to nongrams\n",
    "        \"\"\"\n",
    "        if len(camelCase) > 1:\n",
    "            camelSplit = camelSplit.union(set([RuleMatch(trend=' '.join(camelCase), rule='camel-join')]),\n",
    "                                          set([RuleMatch(trend=' '.join(camelCase).upper(), rule='camel-join-upper')]),\n",
    "                                          set([RuleMatch(trend=' '.join(camelCase).lower(), rule='camel-join-lower')]))\n",
    "\n",
    "        camelCase = set().union(ccHashed, ccHashed_up, ccHashed_lower, ccHashJoined_up, ccHashJoined_lower, ccHashJoined)\n",
    "\n",
    "    onegram = onegram.union(onegram_up, onegram_lower, nohash, nohash_up, nohash_lower, camelCase)\n",
    "\n",
    "    return onegram, camelSplit\n",
    "\n",
    "\n",
    "def nonegram_augment(trend_topic):\n",
    "    \"\"\"\n",
    "    param: trend_topic is a multiple words trend topic\n",
    "\n",
    "    Nongram means a trend consits of more than a single word\n",
    "    transform each word to its upper-lower case\n",
    "    augment with the original set\n",
    "\n",
    "    return: set of augmented-trend-topic\n",
    "\n",
    "    \"\"\"\n",
    "    nonegram = set([RuleMatch(trend=trend_topic, rule='simple')])\n",
    "    nonegram_up = set([RuleMatch(trend=trend_topic.upper(), rule='simple-upper')])\n",
    "    nonegram_lower = set([RuleMatch(trend=trend_topic.lower(), rule='simple-lower')])\n",
    "\n",
    "    return nonegram.union(nonegram_up, nonegram_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_trends(text, onegram_trend_dict, nonegram_trend_dict):\n",
    "    \"\"\"\n",
    "    param: text, twitter text\n",
    "    param: onegram_trend_dict, augmented trend dictionary of onegram trends\n",
    "    param: nonegram_trend_dict, augmented trend dictionary of nonegram trends\n",
    "\n",
    "    For each tweet text, go through augmented trend dictionaries, if there is a match in augmented version\n",
    "    crate a TrendMatch, a namedtuple, which consist of the actual trend, matching version and matching rule\n",
    "\n",
    "    return: set of TrendMatch tuples\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        tokens = set(text.split(' '))\n",
    "        trend_set = set()\n",
    "\n",
    "\n",
    "        ####### Match not only the onegram but with augmented set of it  #########\n",
    "        for onegram, onegram_aug in onegram_trend_dict.items():\n",
    "            if len(onegram_aug) != 0 :\n",
    "                rm = RuleMatch(*zip(*onegram_aug))\n",
    "                aug_list = list(rm.trend)\n",
    "                rules = list(rm.rule)\n",
    "\n",
    "                for ind, aug_trend in enumerate(aug_list):\n",
    "                    if aug_trend in tokens:\n",
    "                        trend_set.add(TrendMatch(trend=onegram, match=aug_trend, matchRule=rules[ind]))\n",
    "\n",
    "\n",
    "        ###### Match not only the nonegram but with augmented set of it  #########\n",
    "        for nonegram, nonegram_aug in nonegram_trend_dict.items():\n",
    "            if len(nonegram_aug) != 0 :\n",
    "                rm = RuleMatch(*zip(*nonegram_aug))\n",
    "                aug_list = list(rm.trend)\n",
    "                rules = list(rm.rule)\n",
    "\n",
    "                for ind, aug_trend in enumerate(aug_list):\n",
    "                    if aug_trend in \" \"+text+\" \":\n",
    "                        trend_set.add(TrendMatch(trend=nonegram, match=aug_trend, matchRule=rules[ind]))\n",
    "\n",
    "\n",
    "        ###### Collect Statistics #####\n",
    "        if len(trend_set) != 0 :\n",
    "            # tm = TrendMatch(*zip(*trend_set))\n",
    "            # unique_trends = set(tm.trend)\n",
    "            # stats.append(len(unique_trends))\n",
    "            return trend_set\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except:\n",
    "        print(\"Text could not be processed: \",  text)\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_trend_set(df, trend_col):\n",
    "    \n",
    "    non_list_cols = [col for col in (df.columns) if col != trend_col ]\n",
    "    df2 = pd.DataFrame(df[trend_col].tolist(), index=[df[col] for col in non_list_cols])\\\n",
    "                    .stack()\\\n",
    "                    .reset_index(name=trend_col)[non_list_cols+[trend_col]]\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_trend_date_indexed_function(file, candidates):\n",
    "    \n",
    "    tweets_folder = TWEETS_PATH\n",
    "    save_folder = SAVE_PATH\n",
    "    \n",
    "    df = pd.read_csv('%s/%s' % (tweets_folder, file))\n",
    "    dfs = []\n",
    "    total_tweet = 0\n",
    "    for candidate in candidates:\n",
    "        df_that_day = pd.DataFrame(df)\n",
    "        trends_that_day = set(trends[trends.date == candidate]['name'])\n",
    "        \n",
    "        total_tweet += df_that_day.shape[0]\n",
    "\n",
    "        if (len(trends_that_day) == 0):\n",
    "            print('trends for %s not found!' % candidate)\n",
    "            continue\n",
    "            \n",
    "        ################################### AUGMENT TREND SETS ############################################\n",
    "        trends_that_day_onegrams = set([trend for trend in trends_that_day if len(trend.split(' ')) == 1])\n",
    "        onegram_trend_dict = dict()\n",
    "        camel_split_dict = dict()\n",
    "        for k in trends_that_day_onegrams:\n",
    "            v1, v2 = onegram_augment(k)\n",
    "            onegram_trend_dict[k] = v1\n",
    "            if len(v2)!=0:\n",
    "                camel_split_dict[k] = v2\n",
    "\n",
    "        \n",
    "        trends_that_day_nonegrams = trends_that_day - trends_that_day_onegrams\n",
    "        nonegram_trend_dict = dict((k, nonegram_augment(k)) for k in trends_that_day_nonegrams)\n",
    "        nonegram_trend_dict.update(camel_split_dict)\n",
    "        \n",
    "        ################################### APPLY TREND INDEX #############################################\n",
    "\n",
    "        df_that_day['TrendMatch'] = df_that_day.text.apply( lambda x: \n",
    "                                    index_trends(x, onegram_trend_dict, nonegram_trend_dict))\n",
    "        df_that_day.dropna(subset=['TrendMatch'], inplace=True)\n",
    "        \n",
    "        df_that_day = expand_trend_set(df_that_day, 'TrendMatch')\n",
    "        df_that_day['trend'] = df_that_day['TrendMatch'].apply(lambda x: x.trend)\n",
    "        df_that_day['match'] = df_that_day['TrendMatch'].apply(lambda x: x.match)\n",
    "        df_that_day['match_rule'] = df_that_day['TrendMatch'].apply(lambda x: x.matchRule)\n",
    "        \n",
    "        df_that_day.drop(['TrendMatch'], axis=1, inplace=True)        \n",
    "        ###################################################################################################\n",
    "        \n",
    "        df_that_day['trend_date'] = candidate\n",
    "        dfs.append(df_that_day)\n",
    "        \n",
    "    \n",
    "    dfs = pd.concat(dfs)\n",
    "    new_file = file.split('_')[0] + \"_trends.csv\"\n",
    "    dfs.to_csv('%s/%s' % (save_folder, new_file), index=False)\n",
    "    statsFile.write(str(total_tweet)+ \",\"+ str(dfs.shape[0]) + \"\\n\")\n",
    "\n",
    "\n",
    "# TO BUILD TREND-DOCS\n",
    "\n",
    "#     dfs = dfs[['text','trends','trend_date']]\\\n",
    "#             .groupby(['trends','trend_date'])['text']\\\n",
    "#             .apply(lambda x: ','.join(x))\\\n",
    "#             .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_trend_date_indexed_parallelized():\n",
    "    \n",
    "    tweets_folder = TWEETS_PATH\n",
    "    save_folder = SAVE_PATH\n",
    "    \n",
    "    files = os.listdir(tweets_folder)\n",
    "    files = [file for file in files if file >= start and file <= end and 'csv' in file] \n",
    "    pool = mp.Pool(mp.cpu_count() - 2)\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        print('%d / %d - %s' % (i, len(files), file))\n",
    "        date = file.split('_')[0]\n",
    "        that_day = pd.Timestamp(date).date()\n",
    "        one_day_before = that_day - pd.Timedelta(days = 1)\n",
    "        one_day_after = that_day + pd.Timedelta(days = 1)\n",
    "        candidates = [str(that_day), str(one_day_before), str(one_day_after)]\n",
    "#         prepare_data_trend_date_indexed_function(file, candidates)\n",
    "        pool.apply_async(prepare_data_trend_date_indexed_function, args=(file, candidates))\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2013-07-07'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trend_date_parser(d):\n",
    "    format_in =  \"%Y-%m-%d %X\"\n",
    "    format_out = \"%Y-%m-%d\"\n",
    " \n",
    "    d = datetime.strptime(d, format_in)\n",
    "    return d.strftime(format_out)\n",
    "\n",
    "trend_date_parser(\"2013-07-07 23:36:32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends = pd.read_csv( os.path.join(TREND_PATH, 'all_trends_world.csv'),\n",
    "                     parse_dates=['date'], date_parser=trend_date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4208889, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>duration</th>\n",
       "      <th>name</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4144722</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>170.0</td>\n",
       "      <td>#RHOP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144723</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>#Bafana</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144724</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>#MondayMorning</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144725</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>#帰れマンデー</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144726</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>#Ranasanaullah</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158917</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>430.0</td>\n",
       "      <td>#NationalMuttDay</td>\n",
       "      <td>10716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158918</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>420.0</td>\n",
       "      <td>#WednesdayThoughts</td>\n",
       "      <td>20892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158919</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>340.0</td>\n",
       "      <td>Godoy Cruz</td>\n",
       "      <td>21496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158920</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>340.0</td>\n",
       "      <td>#DireitaCensurada</td>\n",
       "      <td>37739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158921</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>180.0</td>\n",
       "      <td>İİBFliTUKENDİ UCbinGUY</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  duration                    name   volume\n",
       "4144722 2019-07-01     170.0                   #RHOP      0.0\n",
       "4144723 2019-07-01      10.0                 #Bafana      0.0\n",
       "4144724 2019-07-01     100.0          #MondayMorning      0.0\n",
       "4144725 2019-07-01     100.0                 #帰れマンデー      0.0\n",
       "4144726 2019-07-01     100.0          #Ranasanaullah      0.0\n",
       "...            ...       ...                     ...      ...\n",
       "4158917 2019-07-31     430.0        #NationalMuttDay  10716.0\n",
       "4158918 2019-07-31     420.0      #WednesdayThoughts  20892.0\n",
       "4158919 2019-07-31     340.0              Godoy Cruz  21496.0\n",
       "4158920 2019-07-31     340.0       #DireitaCensurada  37739.0\n",
       "4158921 2019-07-31     180.0  İİBFliTUKENDİ UCbinGUY      0.0\n",
       "\n",
       "[14200 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start =\"2019-06-30\"\n",
    "end = \"2019-08-01\"\n",
    "\n",
    "trends[ (trends.date <'2019-08-01') & (trends.date >= '2019-07-01')]\n",
    "# trends[trends.date <= date & trends.date >= trend_date_parser(start)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_data_trend_date_indexed_parallelized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8937054a734f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstatsFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTATS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stats.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprepare_data_trend_date_indexed_parallelized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstatsFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_data_trend_date_indexed_parallelized' is not defined"
     ]
    }
   ],
   "source": [
    "start =\"2019-07-03\"\n",
    "end = \"2019-07-04\"\n",
    "\n",
    "statsFile = open(os.path.join(STATS_PATH, \"stats.txt\"), \"w+\")\n",
    "prepare_data_trend_date_indexed_parallelized()\n",
    "statsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>duration</th>\n",
       "      <th>name</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4144722</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>170.0</td>\n",
       "      <td>#RHOP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144723</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>#Bafana</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144724</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>#MondayMorning</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144725</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>#帰れマンデー</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144726</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>#Ranasanaullah</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145159</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>250.0</td>\n",
       "      <td>#harrystylesisgoingtojailparty</td>\n",
       "      <td>34662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145160</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>240.0</td>\n",
       "      <td>#야로_시작하는_자동완성</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145161</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>170.0</td>\n",
       "      <td>レイさん</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145162</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>#IndigoEvents</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145163</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>170.0</td>\n",
       "      <td>#Qさま</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  duration                            name   volume\n",
       "4144722 2019-07-01     170.0                           #RHOP      0.0\n",
       "4144723 2019-07-01      10.0                         #Bafana      0.0\n",
       "4144724 2019-07-01     100.0                  #MondayMorning      0.0\n",
       "4144725 2019-07-01     100.0                         #帰れマンデー      0.0\n",
       "4144726 2019-07-01     100.0                  #Ranasanaullah      0.0\n",
       "...            ...       ...                             ...      ...\n",
       "4145159 2019-07-01     250.0  #harrystylesisgoingtojailparty  34662.0\n",
       "4145160 2019-07-01     240.0                   #야로_시작하는_자동완성      0.0\n",
       "4145161 2019-07-01     170.0                            レイさん      0.0\n",
       "4145162 2019-07-01     100.0                   #IndigoEvents      0.0\n",
       "4145163 2019-07-01     170.0                            #Qさま      0.0\n",
       "\n",
       "[442 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trends = pd.read_csv( os.path.join(TREND_PATH, 'all_trends_world.csv'),\n",
    "#                      parse_dates=['date'], date_parser=trend_date_parser)\n",
    "\n",
    "tr31 = set(trends[trends.date == '2019-07-01']['name'])\n",
    "trends[trends.date == '2019-07-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('%s/%s' % (TWEETS_PATH, '2019-09-01_tweetsevenmorebasic.csv.bz2.bz2'))\n",
    "df_that_day = pd.DataFrame(df)\n",
    "# df = df[['text','trends','trend_date']]\\\n",
    "#         .groupby(['trends','trend_date'])['text']\\\n",
    "#         .apply(lambda x: ','.join(x))\\\n",
    "#         .reset_index()\n",
    "df_that_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "onegram = set( [t for t in tr31 if len(t.split(' '))==1 ])\n",
    "onegram.add(\"#EzgiYuceturk\")\n",
    "nonegram = tr31 - onegram\n",
    "\n",
    "onegram_trend_set = dict()\n",
    "camel_split_set = dict()\n",
    "\n",
    "for k in onegram:\n",
    "    v1, v2 = onegram_augment(k)\n",
    "    onegram_trend_set[k] = v1\n",
    "    if len(v2)!=0:\n",
    "        camel_split_set[k] = v2\n",
    "\n",
    "nonegram_trend_set = dict((k, nonegram_augment(k)) for k in nonegram )\n",
    "nonegram_trend_set.update(camel_split_set)\n",
    "stats = []\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{RuleMatch(trend='#EZGI', rule='camel-upper-hashtag'),\n",
       " RuleMatch(trend='#EZGIYUCETURK', rule='simple-upper'),\n",
       " RuleMatch(trend='#Ezgi', rule='camel-hashtag'),\n",
       " RuleMatch(trend='#EzgiYuceturk', rule='simple'),\n",
       " RuleMatch(trend='#YUCETURK', rule='camel-upper-hashtag'),\n",
       " RuleMatch(trend='#Yuceturk', rule='camel-hashtag'),\n",
       " RuleMatch(trend='#ezgi', rule='camel-lower-hashtag'),\n",
       " RuleMatch(trend='#ezgiyuceturk', rule='simple-lower'),\n",
       " RuleMatch(trend='#yuceturk', rule='camel-lower-hashtag'),\n",
       " RuleMatch(trend='EZGI#YUCETURK', rule='camel-upper-join-hashtag'),\n",
       " RuleMatch(trend='EZGIYUCETURK', rule='no-hashtag-upper'),\n",
       " RuleMatch(trend='Ezgi#Yuceturk', rule='camel-join-hashtag'),\n",
       " RuleMatch(trend='EzgiYuceturk', rule='no-hashtag'),\n",
       " RuleMatch(trend='ezgi#yuceturk', rule='camel-lower-join-hashtag'),\n",
       " RuleMatch(trend='ezgiyuceturk', rule='no-hashtag-lower')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onegram_trend_set['#EzgiYuceturk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_that_day['TrendMatch'] = df_that_day.text.apply( lambda x: \n",
    "                            index_trends(x, onegram_trend_set, nonegram_trend_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before: \", df_that_day.shape)\n",
    "df_that_day.dropna(subset=['TrendMatch'], inplace=True)\n",
    "print(\"Before: \", df_that_day.shape)\n",
    "print(\"stats: \", sum(stats)/len(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_that_day = expand_trend_set(df_that_day, 'TrendMatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_that_day['trend'] = df_that_day['TrendMatch'].apply(lambda x: x.trend)\n",
    "df_that_day['match '] = df_that_day['TrendMatch'].apply(lambda x: x.match)\n",
    "df_that_day['match rule'] = df_that_day['TrendMatch'].apply(lambda x: x.matchRule)\n",
    "df_that_day.drop(['TrendMatch'], axis=1, inplace=True)\n",
    "df_that_day.drop(['trends'], axis=1, inplace=True)\n",
    "\n",
    "df_that_day['trend_date'] = '2019-09-01'\n",
    "dfs.append(df_that_day)\n",
    "\n",
    "\n",
    "tweets_folder = TWEETS_PATH\n",
    "save_folder = SAVE_PATH\n",
    "    \n",
    "dfs = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '2019-09-01_trends.csv'\n",
    "new_file = file.split('_')[0] + \"_trends.csv\"\n",
    "dfs.to_csv('%s/%s' % (save_folder, new_file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.text[2])\n",
    "onegram_aug = onegram_trend_set[\"#2cupsstuffed\"]\n",
    "\n",
    "# rm = RuleMatch(*zip(*onegram_aug))\n",
    "# aug_list = list(rm.trend)\n",
    "# rules = list(rm.rule)\n",
    "# print(aug_list) \n",
    "# print(rules)\n",
    "\n",
    "index_trends(df.text[2], onegram_trend_set , nonegram_trend_set)\n",
    "# index_trends(\"My life sucks ezgi yuceturk, forzajuve\", onegram_trend_set , nonegram_trend_set)\n",
    "# index_trends(df.text[20], onegram_trend_set , nonegram_trend_set)\n",
    "# index_trends(df.text[30], onegram_trend_set , nonegram_trend_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
